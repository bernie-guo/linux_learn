## centos7 k8s1.16+kubeedge

## 1 环境介绍

- 1.Master节点（192.168.117.134）
  docker
  etcd
  flannel
  kube-apiserver
  kube-scheduler
  kube-controller-manager

- 2.Node节点（192.168.117.135）
  docker
  flannel
  kubelet
  kube-proxy	
- 3.client节点（）
  客户端
- 4.docker镜像仓库
  阿里云镜像管理

## 2 系统配置

###  2.1 主机设置(所有节点)

- 设置master节点和node节点的主机名
  master执行：hostnamectl --static set-hostname  k8s-master
  node执行：hostnamectl --static set-hostname  k8s-node-1

  edge执行：hostnamectl --static set-hostname  k8s-edge-1

- 修改主机名（所有节点）

```javascript
vi /etc/hosts
192.168.117.147         k8s-master
192.168.117.147         etcd
192.168.117.147         registry
192.168.117.142         k8s-node-1
192.168.117.146         k8s-edge-1
```

- 修改完主机名需要重启一下机器才会生效：reboot



### 2.2 k8s环境准备(所有节点)

安装k8s的机器需要2核和2g内存以上

- 关闭防火墙

  ```
  systemctl stop firewalld
  systemctl disable firewall
  ```

- 关闭selinux、临时禁用selinux

  ```
  setenforce 0
  ```

- 永久关闭 修改/etc/sysconfig/selinux文件设置

  ```
  sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinux
  sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
  ```

- 禁用交换分区

  ```
  swapoff -a
  ```

- 永久禁用，打开/etc/fstab注释掉swap那一行

  ```
  sed -i 's/.*swap.*/#&/' /etc/fstab
  ```

- 修改内核参数

  ```
  cat <<EOF >  /etc/sysctl.d/k8s.conf
  net.bridge.bridge-nf-call-ip6tables = 1
  net.bridge.bridge-nf-call-iptables = 1
  EOF
  ```

- 使配置生效

```javascript
sysctl --system
```

### 2.3 安装docker （所有节点）

```javascript
// 安装docker所需的工具
yum install -y yum-utils device-mapper-persistent-data lvm2
// 配置阿里云的docker源
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
// 指定安装这个版本的docker-ce
yum install -y docker-ce-18.09.9-3.el7
// 启动docker
systemctl enable docker && systemctl start docker
```

### 2.4 安装kubeadm、kubelet、kubectl（node节点安装kubeadm、kubelet）

由于官方k8s源在google，国内无法访问，这里使用阿里云yum源

```javascript
// 执行配置k8s阿里云源
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

// 安装kubeadm、kubectl、kubelet
yum install -y kubectl-1.16.0-0 kubeadm-1.16.0-0 kubelet-1.16.0-0

// 启动kubelet服务
systemctl enable kubelet && systemctl start kubelet
```

## 3 安装k8s

### 3.1 初始化k8s（master节点）

​			以下这个命令开始安装k8s需要用到的docker镜像，因为无法访问到国外网站，所以这条命令使用的是国内的阿里云的源(registry.aliyuncs.com/google_containers)。另一个非常重要的是：这里的–apiserver-advertise-address使用的是master和node间能互相ping通的ip，我这里是192.168.117.147。 这条命令执行时会卡在[preflight] You can also perform this action in beforehand using ''kubeadm config images pull，大概需要2分钟，请耐心等待。

```javascript
// 下载管理节点中用到的6个docker镜像，你可以使用docker images查看到
// 这里需要大概两分钟等待，会卡在[preflight] You can also perform this action in beforehand using ''kubeadm config images pull
kubeadm init --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.16.0 --apiserver-advertise-address 192.168.117.147 --pod-network-cidr=10.244.0.0/16 --token-ttl 0
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200417104604925.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhbjk0OTQxNzE0MA==,size_16,color_FFFFFF,t_70)
上面安装完后，会提示你输入如下命令，复制粘贴过来，执行即可

```javascript
// 上面安装完成后，k8s会提示你输入如下命令，执行
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

init执行成功后需要保存上图 kubeadm join 数据，如果忘记保存可以通过如下命令获取

```javascript
kubeadm token create --print-join-command 
```

查看节点状态，NotReady状态是因为还没有安装网络flanneld，暂时先不用管

```javascript
[root@master resources]# kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health":"true"}

[root@master k8s]# kubectl get node
NAME     STATUS     ROLES    AGE     VERSION
master   NotReady   master   7m22s   v1.18.1
```

集群初始化如果遇到问题，可以使用下面的命令进行清理：

```javascript
kubeadm reset
```

### 3.2 从节点加入集群（所有从节点）

如果忘记加入集群的命令可以登录master节点，使用kubeadm token create --print-join-command 来获取

```javascript
// 加入集群，如果这里不知道加入集群的命令，可以登录master节点，使用kubeadm token create --print-join-command 来获取 
kubeadm join 192.168.117.147:6443 --token e946de.6mplnq51lrj31tvy \
    --discovery-token-ca-cert-hash sha256:a5721369b723a05c85cc5f603776ed89ac2500db11d34653a5a0a5ff1ddfe82f
```

加入成功后，可以在master节点上使用kubectl get nodes命令查看到加入的节点。

### 3.3 安装flannel（master节点）

​		使用wget命令，地址为：(https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml)，这个地址国内访问不了，所以我把内容复制下来，为了避免前面文章过长，我把它粘贴到文章末尾第八个步骤附录了。这个yml配置文件中配置了一个国内无法访问的地址(quay.io)，我已经将其改为国内可以访问的地址(quay-mirror.qiniu.com)。我们新建一个kube-flannel.yml文件，复制粘贴该内容即可。

```javascript
[root@master ~]# kubectl apply -f kube-flannel.yml
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds-amd64 created
daemonset.apps/kube-flannel-ds-arm64 created
daemonset.apps/kube-flannel-ds-arm created
daemonset.apps/kube-flannel-ds-ppc64le created
daemonset.apps/kube-flannel-ds-s390x created
// 状态已经变成Ready
[root@master ~]# kubectl get nodes
NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   17h   v1.16.0
node1    Ready    <none>   17h   v1.16.0
//确保所有status都是running
[root@master ~]# kubectl get pod -n kube-system
NAME                             READY   STATUS    RESTARTS   AGE
coredns-58cc8c89f4-vkjdv         1/1     Running   0          17h
coredns-58cc8c89f4-wrfb7         1/1     Running   0          17h
etcd-master                      1/1     Running   0          17h
kube-apiserver-master            1/1     Running   0          17h
kube-controller-manager-master   1/1     Running   0          17h
kube-flannel-ds-amd64-6rk2s      1/1     Running   0          17h
kube-flannel-ds-amd64-n76pf      1/1     Running   0          17h
kube-proxy-nvw8r                 1/1     Running   0          17h
kube-proxy-rwnm6                 1/1     Running   0          17h
kube-scheduler-master            1/1     Running   0          17h

```

### 3.4 kube-flannel.yml

```javascript
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: psp.flannel.unprivileged
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default
    seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default
    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default
    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default
spec:
  privileged: false
  volumes:
    - configMap
    - secret
    - emptyDir
    - hostPath
  allowedHostPaths:
    - pathPrefix: "/etc/cni/net.d"
    - pathPrefix: "/etc/kube-flannel"
    - pathPrefix: "/run/flannel"
  readOnlyRootFilesystem: false
  # Users and groups
  runAsUser:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  # Privilege Escalation
  allowPrivilegeEscalation: false
  defaultAllowPrivilegeEscalation: false
  # Capabilities
  allowedCapabilities: ['NET_ADMIN']
  defaultAddCapabilities: []
  requiredDropCapabilities: []
  # Host namespaces
  hostPID: false
  hostIPC: false
  hostNetwork: true
  hostPorts:
  - min: 0
    max: 65535
  # SELinux
  seLinux:
    # SELinux is unused in CaaSP
    rule: 'RunAsAny'
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
rules:
  - apiGroups: ['extensions']
    resources: ['podsecuritypolicies']
    verbs: ['use']
    resourceNames: ['psp.flannel.unprivileged']
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes/status
    verbs:
      - patch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flannel
  namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-system
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-amd64
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/os
                    operator: In
                    values:
                      - linux
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
      hostNetwork: true
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
            add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-arm64
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/os
                    operator: In
                    values:
                      - linux
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - arm64
      hostNetwork: true
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay-mirror.qiniu.com/coreos/flannel:v0.11.0-arm64
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay-mirror.qiniu.com/coreos/flannel:v0.11.0-arm64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
             add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-arm
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/os
                    operator: In
                    values:
                      - linux
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - arm
      hostNetwork: true
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay-mirror.qiniu.com/coreos/flannel:v0.11.0-arm
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay-mirror.qiniu.com/coreos/flannel:v0.11.0-arm
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
             add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-ppc64le
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/os
                    operator: In
                    values:
                      - linux
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - ppc64le
      hostNetwork: true
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay-mirror.qiniu.com/coreos/flannel:v0.11.0-ppc64le
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay-mirror.qiniu.com/coreos/flannel:v0.11.0-ppc64le
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
             add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-s390x
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/os
                    operator: In
                    values:
                      - linux
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - s390x
      hostNetwork: true
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay-mirror.qiniu.com/coreos/flannel:v0.11.0-s390x
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay-mirror.qiniu.com/coreos/flannel:v0.11.0-s390x
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
             add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg
```

### 3.5 recommended.yaml

```javascript
# Copyright 2017 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: v1
kind: Namespace
metadata:
  name: kubernetes-dashboard

---

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard

---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 32443
  selector:
    k8s-app: kubernetes-dashboard

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-certs
  namespace: kubernetes-dashboard
type: Opaque

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-csrf
  namespace: kubernetes-dashboard
type: Opaque
data:
  csrf: ""

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-key-holder
  namespace: kubernetes-dashboard
type: Opaque

---

kind: ConfigMap
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-settings
  namespace: kubernetes-dashboard

---

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
rules:
  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
  - apiGroups: [""]
    resources: ["secrets"]
    resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs", "kubernetes-dashboard-csrf"]
    verbs: ["get", "update", "delete"]
    # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["kubernetes-dashboard-settings"]
    verbs: ["get", "update"]
    # Allow Dashboard to get metrics.
  - apiGroups: [""]
    resources: ["services"]
    resourceNames: ["heapster", "dashboard-metrics-scraper"]
    verbs: ["proxy"]
  - apiGroups: [""]
    resources: ["services/proxy"]
    resourceNames: ["heapster", "http:heapster:", "https:heapster:", "dashboard-metrics-scraper", "http:dashboard-metrics-scraper"]
    verbs: ["get"]

---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
rules:
  # Allow Metrics Scraper to get metrics from the Metrics server
  - apiGroups: ["metrics.k8s.io"]
    resources: ["pods", "nodes"]
    verbs: ["get", "list", "watch"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard

---

kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      containers:
        - name: kubernetes-dashboard
          image: kubernetesui/dashboard:v2.0.0-beta4
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8443
              protocol: TCP
          args:
            - --auto-generate-certificates
            - --namespace=kubernetes-dashboard
            # Uncomment the following line to manually specify Kubernetes API server Host
            # If not specified, Dashboard will attempt to auto discover the API server and connect
            # to it. Uncomment only if the default does not work.
            # - --apiserver-host=http://my-address:port
          volumeMounts:
            - name: kubernetes-dashboard-certs
              mountPath: /certs
              # Create on-disk volume to store exec logs
            - mountPath: /tmp
              name: tmp-volume
          livenessProbe:
            httpGet:
              scheme: HTTPS
              path: /
              port: 8443
            initialDelaySeconds: 30
            timeoutSeconds: 30
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1001
            runAsGroup: 2001
      volumes:
        - name: kubernetes-dashboard-certs
          secret:
            secretName: kubernetes-dashboard-certs
        - name: tmp-volume
          emptyDir: {}
      serviceAccountName: kubernetes-dashboard
      nodeSelector:
        "kubernetes.io/os": linux
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule

---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: dashboard-metrics-scraper
  name: dashboard-metrics-scraper
  namespace: kubernetes-dashboard
spec:
  ports:
    - port: 8000
      targetPort: 8000
  selector:
    k8s-app: dashboard-metrics-scraper

---

kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: dashboard-metrics-scraper
  name: dashboard-metrics-scraper
  namespace: kubernetes-dashboard
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: dashboard-metrics-scraper
  template:
    metadata:
      labels:
        k8s-app: dashboard-metrics-scraper
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'runtime/default'
    spec:
      containers:
        - name: dashboard-metrics-scraper
          image: kubernetesui/metrics-scraper:v1.0.4
          ports:
            - containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              scheme: HTTP
              path: /
              port: 8000
            initialDelaySeconds: 30
            timeoutSeconds: 30
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1001
            runAsGroup: 2001
      serviceAccountName: kubernetes-dashboard
      nodeSelector:
        "kubernetes.io/os": linux
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
      volumes:
        - name: tmp-volume
          emptyDir: {}
```



## 4 应用部署

#### 流程图

​	制作镜像-->控制器管理pod-->暴露应用-->对外发布应用-->日志/监控

#### 使用Deployment部署Java应用

[tomcat-java-demo-master](https://www.centoscn.vip/wp-content/uploads/2020/09/tomcat-java-demo-master.zip)

- 上传源代码到服务器

```
[root@k8s-master1 tomcat-java-demo-master]# pwd 
/root/tomcat-java-demo-master
[root@k8s-master1 tomcat-java-demo-master]# ||
total 32
drwx------ 2 root root 4096 Sep 15 19:41 db
-rw------- 1 root root 148 Aug 4 2019 Dockerfile
-rw------- 1 root root 11357 Aug 4 2019 LICENSE
-rw------- 1 root root 1930 Aug 4 2019 pom.xml
-rw------- 1 root root 270 Aug 4 2019 README.md
drwx------ 3 root root 4096 Sep 15 19:41 src
```



#### 镜像分类：

- 基础镜像，例如centos，ubuntu
- 环境镜像，例如jdk，python，go
- 项目镜像，是将项目打包到里面



#### 制作镜像

- 由于要编译打包，先安装基础环境jdk和maven

```
[root@k8s-master1 ~]# yum install java-1.8.0-openjdk maven -y
```

- 编译构建(构建新的包，跳过单元测试）

```
[root@k8s-master1 tomcat-java-demo-master]# pwd
/root/tomcat-java-demo-master
[root@k8s-master1 tomcat-java-demo-master]# mvn clean package -D maven.skip.test=true
```

- 编译完成，会生成一个target目录，里面就是我们打包完成打war包

```
[root@k8s-master1 tomcat-java-demo-master]# ||
total 36
drwx------ 2 root root 4096 Sep 15 19:41 db
-rw------- 1 root root 148 Aug 4 2019 Dockerfile
-rw------- 1 root root 11357 Aug 4 2019 LICENSE
-rw------- 1 root root 1930 Aug 4 2019 pom.xml
-rw------- 1 root root 270 Aug 4 2019 README.md
drwx------ 3 root root 4096 Sep 15 19:41 src
drwxr-xr-x 7 root root 4096 Sep 18 20:11 target
```

- 编写Dockerfile

```
[root@k8s-master1 tomcat-java-demo-master]# vim Dockerfile 
FROM tomcat:8 ###使用官方tomcat8的镜像
LABEL maintainer www.centoscn.vip ###打个标签
ADD target/*.war /usr/local/tomcat/webapps/ROOT.war  #####将刚才编译打包后的war包，拷贝到镜像的这个目录下
```

- 构建镜像

```
[root@k8s-master1 tomcat-java-demo-master]# docker build -t java-demo:v1 .
```

- 查看构建完成的镜像

```
[root@k8s-master1 tomcat-java-demo-master]# docker images
REPOSITORY TAG IMAGE ID CREATED SIZE
java-demo v1 38871248ae51 23 seconds ago 548MB
```

- 上传镜像到仓库（比如阿里云的镜像仓库，hub等等）

```
[root@k8s-master1 tomcat-java-demo-master]# docker login
Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.
Username: centoscn
Password: 
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store
 
Login Succeeded
```

- 打tag号上传到Hub上

```
[root@k8s-master1 ~]# docker tag java-demo:v1 centoscn/java-demo:v1
```

- 推送镜像到远程仓库

```
[root@k8s-master1 ~]# docker push centoscn/java-demo:v1
The push refers to repository [docker.io/centoscn/java-demo]
```

#### Deployment控制器功能

- 部署无状态应用
  管理pod和replicaset
  具有上线部署，副本设定，滚动升级，回滚等功能。
  提供声明式更新，例如只更新一个新的lmage
  应用场景：Web服务，微服务



#### 控制器管理pod

- 由于之前使用web做过测试，直接删除掉

```
[root@k8s-master1 ~]# kubectl delete deploy web
deployment.apps "web" deleted
[root@k8s-master1 ~]# kubectl delete svc web
service "web" deleted
```

- 创建

```
[root@k8s-master1 ~]# kubectl create deployment web --image=centoscn/java-demo:v1
deployment.apps/web created
[root@k8s-master1 ~]# kubectl get pods
NAME READY STATUS RESTARTS AGE
dns-test 1/1 Running 2 5d14h
web-b9b864798-l7nrp 1/1 Running 0 10s
```

- 发布出去，暴露端口

```
[root@k8s-master1 ~]# kubectl expose deployment web --port=80 --type=NodePort --target-port=8080 --name=web
service/web exposed
```

- 查看service

```
[root@k8s-master1 ~]# kubectl get service
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubernetes ClusterIP 10.0.0.1 <none> 443/TCP 5d15h
web NodePort 10.0.0.197 <none> 80:30230/TCP 11s
```

- 使用curl请求测试

```
[root@k8s-master1 ~]# curl 10.0.0.197 
<!DOCTYPE html>
<html>
<head lang="en">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>把美女带回家应用案例</title>
<meta name="description" content="把美女带回家应用案例">
<meta name="keywords" content="index">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="renderer" content="webkit">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta http-equiv="Cache-Control" content="no-siteapp" />
<link rel="stylesheet" href="../js/layui/css/layui.css" media="all">
<link rel="stylesheet" href="../css/reset.css">
<link rel="stylesheet" href="../css/supersized.css">
<link rel="stylesheet" href="../css/style.css">
<link rel="stylesheet" href="../css/typeit.css">
<link rel="stylesheet" href="../css/date.css">
</head>
<body>
<!-- <img src="../images/logo.png"> -->
<div class="page-container">
<h1 class="title"></h1>
<div style="margin-top: 15%;">
<button class="layui-btn layui-btn-lg layui-btn-primary layui-btn-radius" onclick="adduser()">添加美女</button>
<button class="layui-btn layui-btn-lg layui-btn-primary layui-btn-radius" onclick="queryuserList()">今晚翻盘哪个</button>
</div>
```

- 浏览器测试

![浏览器测试](https://www.centoscn.vip/wp-content/uploads/2020/09/Snip20200919_3.png

- 查看pod详细信息

```
[root@k8s-master1 ~]# kubectl get pods -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
dns-test 1/1 Running 2 5d14h 10.244.0.11 k8s-node1 <none> <none>
web-b9b864798-bzwgk 1/1 Running 0 3m30s 10.244.2.5 k8s-master1 <none> <none>
```

- 查看ep

```
[root@k8s-master1 ~]# kubectl get ep
NAME ENDPOINTS AGE
kubernetes 10.10.1.37:6443,10.10.1.38:6443 5d15h
web 10.244.2.5:8080 3m55s
```

- 扩容副本

```
[root@k8s-master1 ~]# kubectl scale deployment web --replicas=3
deployment.apps/web scaled
```

- 查看节点详细信息

```
[root@k8s-master1 ~]# kubectl get pod -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
dns-test 1/1 Running 2 5d15h 10.244.0.11 k8s-node1 <none> <none>
web-b9b864798-bzwgk 1/1 Running 0 9m12s 10.244.2.5 k8s-master1 <none> <none>
web-b9b864798-d5qgt 0/1 ContainerCreating 0 2m13s <none> k8s-master2 <none> <none>
web-b9b864798-hsgd8 0/1 ContainerCreating 0 2m13s <none> k8s-node2 <none> <none>
```

- 查看为啥还没部署成功，发现是在下载镜像

```
[root@k8s-master1 ~]# kubectl describe pod web-b9b864798-d5qgt
Name: web-b9b864798-d5qgt
Namespace: default
Priority: 0
Node: k8s-master2/10.10.1.38
Start Time: Sat, 19 Sep 2020 14:57:29 +0800
Labels: app=web
pod-template-hash=b9b864798
Annotations: <none>
Status: Pending
IP: 
IPs: <none>
Controlled By: ReplicaSet/web-b9b864798
Containers:
java-demo:
Container ID: 
Image: centoscn/java-demo:v1
Image ID: 
Port: <none>
Host Port: <none>
State: Waiting
Reason: ContainerCreating
Ready: False
Restart Count: 0
Environment: <none>
Mounts:
/var/run/secrets/kubernetes.io/serviceaccount from default-token-6lgmm (ro)
Conditions:
Type Status
Initialized True 
Ready False 
ContainersReady False 
PodScheduled True 
Volumes:
default-token-6lgmm:
Type: Secret (a volume populated by a Secret)
SecretName: default-token-6lgmm
Optional: false
QoS Class: BestEffort
Node-Selectors: <none>
Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s
node.kubernetes.io/unreachable:NoExecute for 300s
Events:
Type Reason Age From Message
---- ------ ---- ---- -------
Normal Scheduled <unknown> default-scheduler Successfully assigned default/web-b9b864798-d5qgt to k8s-master2
Normal Pulling 2m16s kubelet, k8s-master2 Pulling image "centoscn/java-demo:v1"
```

## 5 kubeedge准备

**环境**

- 云端：centos7，用户名为k8s-master。ip 为 192.168.179.131 
- 边缘端：centos7，用户名为k8-edge-1。ip 为 192.168.179.142
- KubeEdge 部署需要的组件：（注意，**本安装教程假定你已经在云端机器上安装好了k8s集群和docker，并在边缘端机器上安装好了docker**）
- 云端：docker， kubernetes 集群和 KubeEdge 云端核心模块。
- 边缘端：docker， mqtt 和 KubeEdge 边缘端核心模块。

**依赖**

- golang：版本1.12.14，移步 https://studygolang.com/dl 下载（由于是源码编译，所以需要用到golang）
- k8s版本：v1.16.0
- mosquitto：安装和使用
- KubeEdge：v1.1.0



### 5.1 创建部署文件目录

本安装方法主要适用于初学者，因为初学者对于搭建集群往往是比较陌生的。本文会先创建一个目录，存放各类安装过程中需要的文件。

创建部署工程目录：

```
# mkdir /home/kkbill/kubeedge
```

创建子目录： 

```
# cd /home/kkbill/kubeedge

# mkdir cloud edge certs yamls src
```

- 说明：

  - cloud：云端相关文件，包括 cloudcore 和配置文件。
  - edge：边缘端相关文件，包括 edgecore 和配置文件。
  - certs：证书文件。
  - yamls：一些 yamls 文件。
  - src：源码目录，存放kubeedge源码。

- 

  ### **5.2 KubeEdge 二进制**

获取KubeEdge的方式有两种，一种是直接从 官网(https://github.com/kubeedge/kubeedge/releases) 中下载（本实验版本为kubeedge-v1.1.0-linux-amd64.tar.gz）；另一种方法是通过源码编译得到。这里介绍一下源码编译的方法（嗯，折腾...）

**2.2.1 Golang环境搭建**

(1) 下载golang，并解压

```
# wget https://studygolang.com/dl/golang/go1.12.14.linux-amd64.tar.gz

# tar -C /usr/local -xzf  go1.12.14.linux-amd64.tar.gz
```

(2) 添加环境变量

在~/.bashrc文件末尾添加：

```
# vim ~/.bashrc

export GOPATH=/home/kkbill/kubeedge
export PATH=$PATH:/usr/local/go/bin
```

保存后记得执行 source ~/.bashrc 生效。验证：

```
# go version
# go version go1.12.14 linux/amd64
```

**2.2.2 下载kubeedge源码**

```
# git clone https://github.com/kubeedge/kubeedge.git $GOPATH/src/github.com/kubeedge/kubeedge
```

**2.2.3 检测gcc是否安装**

```
# gcc --version
```

如果没有，则自行安装。

**2.2.4 编译云端**

```
# cd $GOPATH/src/github.com/kubeedge/kubeedge/

# make all WHAT=cloudcore
```

生成二进制 cloudcore 文件位于 cloud 目录。拷贝 cloudcore 和同一目录的配置文件（conf目录）到部署工程目录：

```
# cp -a cloud/cloudcore $GOPATH/cloud/
# cp -a cloud/conf/ $GOPATH/cloud/
```

在编译的时候遇到了第一个坑，就是版本的问题。由于最新clone下来的版本已经不是v1.1.0了，所以，我们需要把代码切回到v1.1.0版本，操作如下：（如果你在这一步无异常，则跳过）

在 $GOPATH/src/github.com/kubeedge/kubeedge 目录下，执行 git tag，并选择 v1.1.0 即可

```
# git tag
...
v1.1.0
...

# git checkout v1.1.0 
```

执行完这一句后，代码就会回到v1.1.0版。

**2.2.5 编译边缘端**

```
# cd $GOPATH/src/github.com/kubeedge/kubeedge/

# make all WHAT=edgecore
```

生成二进制 edgecore 文件位于 edge 目录。拷贝二进制及配置文件到部署工程目录：

```
# cp -a edge/edgecore $GOPATH/edge/

# cp -a edge/conf/ $GOPATH/edge/
```

这里又遇到了第2个坑，即出现如下错误：

```
/usr/local/go/pkg/tool/linux_amd64/link: signal: killed
```

这是由于编译需要较大的内存，而内存不够，造成了OOM。对应的解决办法：增加内存

(1) 创建要作为swap分区的文件:增加1GB大小的交换分区，则命令写法如下，其中的count等于想要的块的数量（bs*count=文件大小）。

```
# dd if=/dev/zero of=/root/swapfile bs=1M count=1024
```

(2) 格式化为交换分区文件:

```
mkswap /root/swapfile #建立swap的文件系统
```

(3) 启用交换分区文件:

```
swapon /root/swapfile #启用swap文件
```

解决这个问题，参考了：

```
https://forum.golangbridge.org/t/go-build-exits-with-signal-killed/513

https://segmentfault.com/a/1190000012219689

https://www.cnblogs.com/spjy/p/7085389.html
```



### **5.3 生成证书**

```
# $GOPATH/src/github.com/kubeedge/kubeedge/build/tools/certgen.sh genCertAndKey edge

# cp -a /etc/kubeedge/* $GOPATH/certs
```

生成的 ca 和 certs 分别位于 /etc/kubeedge/ca 和 /etc/kubeedge/certs 目录，将其拷贝到部署工程目录的 certs 目录。注意，这是在云端机器执行，所以云端已经有了证书，拷贝到 certs 目录，是为了方便分发到边缘节点。



### **5.4 拷贝设备模块和设备CRD yaml 文件**

```
cp $GOPATH/src/github.com/kubeedge/kubeedge/build/crds/devices/* $GOPATH/yamls
```



### **5.5 拷贝node.json**

```
cp $GOPATH/src/github.com/kubeedge/kubeedge/build/node.json $GOPATH/cloud
```

释义：node.json 为节点的配置信息，需要在云端机器执行，作用是将边缘端加入集群（但实际上只是让 k8s 知道有这个节点，还不是真正意义上的加入）



### **5.6 配置云端节点**

打开配置文件 $GOPATH/cloud/conf/controller.yaml ，修改两处 master 的值，将master修改为 k8s 的apiserver的地址，在我的配置中，修改为：https://192.168.179.131:6443

这个地址当然就是云端这台机器的ip，那么端口是怎么来的？你可以从 /etc/kubernetes/manifests/kube-apiserver.yaml 中找到答案，如下图所示：

（当然，这里的前提是你已经装好了kubernetes，但是安装 kubernetes 不在本教程范围。）

 ![img](https://img2020.cnblogs.com/blog/1442950/202003/1442950-20200330213223439-1561103334.png)

 这里提醒一下，我在第一次安装的时候，由于自身对k8s也还非常的不了解，还不知道是查看kube-apiserver.yaml，所以错写成了http，这个错误让我废了很长的时间才解决。

另外，还需要将kubeconfig的值修改为：/root/.kube/config

![img](https://img2020.cnblogs.com/blog/1442950/202003/1442950-20200330214208350-1616304629.png)

 这一步也存在一个坑，如果设置不正确，可能会导致 cloudcore 启动失败，出现如下错误：

E0102 17:53:58.630318 15021 reflector.go:125] github.com/kubeedge/kubeedge/cloud/pkg/devicecontroller/manager/device.go:40: Failed to list *v1alpha1.Device: the server rejected our request for an unknown reason (get devices.devices.kubeedge.io)
E0102 17:53:58.630335 15021 reflector.go:125] github.com/kubeedge/kubeedge/cloud/pkg/devicecontroller/manager/devicemodel.go:40: Failed to list *v1alpha1.DeviceModel: the server rejected our request for an unknown reason (get devicemodels.devices.kubeedge.io)
W0102 17:53:58.632435 15021 controller.go:51] new downstream controller failed with error: the server rejected our request for an unknown reason (get nodes)

这一问题在 https://github.com/kubeedge/kubeedge/issues/1361 有讨论，我也是从这个issue中摸索出了自己的解决方案。



### **5.7 配置边缘节点**

打开配置文件$GOPATH/edge/conf/edge.yaml ，将如下两处ip换成你自己的云端主机的ip。

![img](https://img2020.cnblogs.com/blog/1442950/202003/1442950-20200330215031057-2107395820.png)

 另外，由于在v1.1.0版本中，边缘节点名称为 `fb4ebb70-2783-42b8-b3ef-63e2fd6d242e，这不方便阅读，我们将其修改为edge-node。在edge.yaml文件中把所有 fb4ebb70-2783-42b8-b3ef-63e2fd6d242e 的值替换为 edge-node。`

此外，还有非常重要的一点是，注意该文件中 cgroup-driver 字段的值，默认情况下是 cgroupfs，有些文章不说明具体原因就让你将其修改为 sysemd，这是不负责的。**这里是否需要修改该字段的值取决于你安装的docker的cgroup-driver是什么，原则就是要保持两者一直**，若不一致，就会出现致命的问题。

可以通过 docker info 命令查看已安装的docker的cgroup-driver，确定好后，再决定是否修改edge.yaml 文件中cgroup-driver的值。

这里我也踩过坑，第一次安装时未经检查，就盲目修改了该字段的值，导致最后边缘节点的状态始终是NotReady，这个问题在issue列表中也有讨论，详见 https://github.com/kubeedge/kubeedge/issues/1243 。



### **5.8 安装mqtt**

###### 1、mqtt只需要在边缘端安装。由于我使用的是ubuntu系统，直接使用apt-get，如下：

\# add-apt-repository ppa:mosquitto-dev/mosquitto-ppa // 添加源
\# apt-get update // 更新
\# apt-get install mosquitto // 安装

2、centos安装

- 1 安装

截止2017年12月，最新版本为mosquitto-1.4.14

```
# 下载源代码包
wget http://mosquitto.org/files/source/mosquitto-1.4.14.tar.gz
# 解压
tar zxfv mosquitto-1.4.14.tar.gz
# 进入目录
cd mosquitto-1.4.14
# 编译
make
# 安装
sudo make instal12345678910
```

- 2 问题

【1】编译过程找不到ares.h

```
sudo yum install c-ares-devel1
```

【2】编译过程中找不到libwebsockets.h

```
sudo yum -y install libwebsockets-devel.x86_641
```

【3】使用过程中找不到libmosquitto.so.1
error while loading shared libraries: libmosquitto.so.1: cannot open shared object file: No such file or directory
【解决方法】——修改libmosquitto.so位置
创建链接

```
sudo ln -s /usr/local/lib/libmosquitto.so.1 /usr/lib/libmosquitto.so.11
```

更新动态链接库

```
sudo ldconfig
```



## 6 部署

前面已经准备好了所需要的配置文件，现在，只需要将证书和边缘端的文件拷贝到边缘机器上，使用 scp 命令进行拷贝。（当然，你也可以在边缘机器上完成上面的2.7节。）

在云端主机上进行操作，192.168.179.142是边缘端的ip。

```
# ssh 192.168.117.145 "mkdir -p /etc/kubeedge /home/kkbill/kubeedge/edge"
# scp -r $GOPATH/certs/* 192.168.117.145:/etc/kubeedge
# scp -r $GOPATH/edge/* 192.168.117.145:/home/kkbill/kubeedge/edge
```

这里可能会涉及到一些permission deny的问题，这是由于 scp 命令造成的，这个问题不难，自己解决一下。



### **6.1 云端**

首先进入部署工程目录：cd /home/kkbill/kubeedge

**1、 添加边缘端到集群**

```
# kubectl apply -f cloud/node.json
```

**注意**：在执行该命令前，务必修改node.json文件的值，如下，把name字段的值由 fb4ebb70-2783-42b8-b3ef-63e2fd6d242e 的值替换为 edge-node。

![img](https://img2020.cnblogs.com/blog/1442950/202003/1442950-20200330224140558-2135133133.png)

 此时，查看节点状态，边缘端未就绪。如下：

```
# kubectl get nodes 

NAME        STATUS      ROLES   AGE     VERSION
edge-node NotReady    edge       9s
k8smaster  Ready          master    15h       v1.16.0
```

**2、 创建设备模块和设备CRD**

```
# kubectl apply -f yamls
```

 注意，yamls 目录有2个 yaml 文件，此处指定目录，会自动执行目录所有文件。

**3、运行cloudcore**

```
# cd  cloud

# ./cloudcore
```

可能会出现如下错误：

```
github.com/kubeedge/kubeedge/cloud/pkg/edgecontroller/manager/secret.go:31: Failed to list *v1.Secret: Get https://192.168.179.131:6443/api/v1/secrets?limit=500&resourceVersion=0: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")
```

参考这里的讨论 https://github.com/kubernetes/kubernetes/issues/48378 ，我忘了当时是怎么解决这个问题的了...



### **6.2 边缘端**

该部分在边缘端机器上进行。

首先，在边缘端运行mqtt：

```
# mosquitto -d -p 1883
```

然后进入部署工程目录：cd /home/kkbill/kubeedge/edge，并运行 edgecore：

```
# ./edgecore
```



### **6.3 验证**

在云端查看状态：

```
# kubectl get nodes
NAME        STATUS     ROLES    AGE    VERSION
edge-node   Ready         edge        8h       v1.15.3-kubeedge-v1.1.0
k8smaster    Ready        master     173d   v1.16.0
```

这个时候，edge-node 的状态就变成了 Ready。到这里，基本上算是搭建完成了。



## 7 Cloud 部署kubeedge-web-app

### 7.1 确认集群状态

```
su #切换root用户
```

运行

```
kubectl get nodes
```

输出

```
NAME                 STATUS   ROLES    AGE   VERSION
kind-control-plane   Ready    master   19h   v1.17.2
test1                Ready    edge     19h   v1.17.1-kubeedge-v1.2.1
```

运行

```
docker ps
```

输出

```
CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                      NAMES
309a72ab9dad        kindest/node:v1.17.2   "/usr/local/bin/entr…"   19 hours ago        Up 2 hours          127.0.0.1:6443->6443/tcp   kind-control-plane
```

### 7.2下载编译example代码

- 建议scp

- 建议VS code remote

  1. 下载代码

  ```
  git clone https://github.com/kubeedge/examples $GOPATH/src/github.com/kubeedge/examples
  ```

  1. 构建基础镜像

  ```
  cd $GOPATH/src/github.com/kubeedge/examples
  cd kubeedge-web-demo/kubeedge-web-app/
  make all
  docker pull centos:7.6.1810
  docker build . -t kubeedge/kubeedge-web-app:v2.6
  ```

### 7.3 修改kubeedge-web-demo代码

**以下命令$GOPATH/src/github.com/kubeedge/examples/kubeedge-web-demo为根目录**

1. 修改Dockerfile代码

   文件:

   ```
   kubeedge-web-demo/kubeedge-web-app/Dockerfile
   ```

   修改结果:

   ```
   # Based on centos
   FROM kubeedge/kubeedge-web-app:v2.6
   LABEL maintainers="KubeEdge Authors"
   LABEL description="KubeEdge Web App"
   
   # Copy from build directory
   COPY kubeedge-web-app /kubeedge-web-app
   COPY static /static
   COPY views /views
   
   # Update
   RUN yum -y update
   
   # Define default command
   ENTRYPOINT ["/kubeedge-web-app"]
   
   # Run the executable
   CMD ["kubeedge-web-app"]
   ```

2. 修改后端

   文件:

   ```
   kubeedge-web-demo/kubeedge-web-app/utils/kubeclient.go
   ```

   修改结果:

   ```
   package utils
   
   import (
   	"errors"
   	"fmt"
   	"io/ioutil"
   	"k8s.io/client-go/rest"
   	certutil "k8s.io/client-go/util/cert"
   	"net"
   	"os"
   )
   
   var KubeQPS = float32(5.000000)
   var KubeBurst = 10
   var KubeContentType = "application/vnd.kubernetes.protobuf"
   var ErrNotInCluster = errors.New("unable to load in-cluster configuration, KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT must be defined")
   
   func InClusterConfig() (*rest.Config, error) {
   	const (
   		tokenFile  = "/var/run/secrets/kubernetes.io/serviceaccount/token"
   		rootCAFile = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
   	)
   	host, port := os.Getenv("KUBERNETES_SERVICE_HOST"), os.Getenv("KUBERNETES_SERVICE_PORT")
   	if len(host) == 0 || len(port) == 0 {
   		return nil, ErrNotInCluster
   	}
   
   	token, err := ioutil.ReadFile(tokenFile)
   	if err != nil {
   		return nil, err
   	}
   
   	tlsClientConfig := rest.TLSClientConfig{}
   
   	if _, err := certutil.NewPool(rootCAFile); err != nil {
   		fmt.Errorf("Expected to load root CA config from %s, but got err: %v", rootCAFile, err)
   	} else {
   		tlsClientConfig.CAFile = rootCAFile
   	}
   
   	return &rest.Config{
   		// TODO: switch to using cluster DNS.
   		Host:            "https://" + net.JoinHostPort(host, port),
   		TLSClientConfig: tlsClientConfig,
   		BearerToken:     string(token),
   	}, nil
   }
   
   // KubeConfig from flags
   func KubeConfig() (conf *rest.Config, err error) {
   	kubeConfig, err := InClusterConfig()
   	if err != nil {
   		return nil, err
   	}
   	kubeConfig.QPS = KubeQPS
   	kubeConfig.Burst = KubeBurst
   	kubeConfig.ContentType = KubeContentType
   	return kubeConfig, err
   }
   ```

3. 修改deployments

   文件:

   ```
   kubeedge-web-demo/kubeedge-web-app/deployments/kubeedge-speaker-instance.yaml
   ```

   修改结果:

   ```
   apiVersion: devices.kubeedge.io/v1alpha1
   kind: Device
   metadata:
     name: speaker-01
     labels:
       description: 'Speaker'
       manufacturer: 'test'
   spec:
     deviceModelRef:
       name: speaker-model
     nodeSelector:
       nodeSelectorTerms:
       - matchExpressions:
         - key: ''
           operator: In
           values:
           - test1
   ```

   文件:

   ```
   kubeedge-web-demo/kubeedge-web-app/deployments/kubeedge-web-app.yaml
   ```

   修改结果:

   ```
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     labels:
       k8s-app: kubeedge-web-app
     name: kubeedge-web-app
     namespace: default
   spec:
     selector:
       matchLabels:
         k8s-app: kubeedge-web-app
     template:
       metadata:
         labels:
           k8s-app: kubeedge-web-app
       spec:
         nodeSelector:
           node-role.kubernetes.io/master: ""
         hostNetwork: true
         containers:
         - name: kubeedge-web-app
           image: kubeedge/kubeedge-web-app:v2.7
           imagePullPolicy: IfNotPresent
         restartPolicy: Always
   ```

### 7.4 编译Docker镜像, 部署Deployment, 运行Pods

1. 编译Docker镜像

   ```
   cd $GOPATH/src/github.com/kubeedge/examples
   cd kubeedge-web-demo/kubeedge-web-app/
   make clean && make all
   docker build . -t kubeedge/kubeedge-web-app:v2.7
   ```

   检查Docker镜像

   ```
   docker images
   ```

   输出

   ```
   REPOSITORY                  TAG                 IMAGE ID            CREATED             SIZE
   kubeedge/kubeedge-web-app   v2.7                e7e17d876279        21 hours ago        626MB
   kubeedge/kubeedge-web-app   v2.6                a78a95028de8        23 hours ago        488MB
   kindest/node                v1.17.2             df953f7b867a        8 weeks ago         1.25GB
   centos                      7.6.1810            f1cb7c7d58b7        12 months ago       202MB
   ```

   1. 修改集群用户权限

   资料:https://github.com/fabric8io/fabric8/issues/6840:

   创建资源文件:

   ```
   sudo tee /root/fabric8-rbac.yaml <<-'EOF'
   apiVersion: rbac.authorization.k8s.io/v1beta1
   kind: ClusterRoleBinding
   metadata:
     name: fabric8-rbac
   subjects:
     - kind: ServiceAccount
       # Reference to upper's `metadata.name`
       name: default
       # Reference to upper's `metadata.namespace`
       namespace: default
   roleRef:
     kind: ClusterRole
     name: cluster-admin
     apiGroup: rbac.authorization.k8s.io
   EOF
   ```

   部署资源文件:

   ```
   kubectl create -f /root/fabric8-rbac.yaml
   ```

   1. 部署Docker镜像为Deployments

   上传镜像到集群

   资料:https://kind.sigs.k8s.io/docs/user/known-issues/#unable-to-pull-images

   ```
   kind load docker-image kubeedge/kubeedge-web-app:v2.7
   ```

   部署

   ```
   cd $GOPATH/src/github.com/kubeedge/examples
   cd kubeedge-web-demo/kubeedge-web-app/deployments
   kubectl create -f kubeedge-speaker-model.yaml
   kubectl create -f kubeedge-speaker-instance.yaml
   kubectl create -f kubeedge-web-app.yaml
   ```

### 7.5 检查集群运行状态

```
$ kubectl get nodes
NAME                 STATUS   ROLES    AGE   VERSION
kind-control-plane   Ready    master   20h   v1.17.2
test1                Ready    edge     20h   v1.17.1-kubeedge-v1.2.1
$ docker ps
CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                      NAMES
309a72ab9dad        kindest/node:v1.17.2   "/usr/local/bin/entr…"   21 hours ago        Up 7 minutes        127.0.0.1:6443->6443/tcp   kind-control-plane
$ kubectl get crds
NAME                                           CREATED AT
clusterobjectsyncs.reliablesyncs.kubeedge.io   2020-04-04T13:25:40Z
devicemodels.devices.kubeedge.io               2020-04-04T13:25:40Z
devices.devices.kubeedge.io                    2020-04-04T13:25:39Z
objectsyncs.reliablesyncs.kubeedge.io          2020-04-04T13:25:40Z
$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
kubeedge-web-app   1/1     1            1           20h
$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
kubeedge-web-app-76d4755f59-f5zbr   1/1     Running   2          20h
$ kubectl describe pod kubeedge-web-app-76d4755f59-f5zbr | grep IP
IP:           172.17.0.2
IPs:
  IP:           172.17.0.2
```

在Cloud上访问上一步得到的IP, 可以看到如下网页, 网页不用关: [![page](https://github.com/JingruiLea/blogs/raw/master/img/page.png)](https://github.com/JingruiLea/blogs/blob/master/img/page.png)

## 8 Edge 部署客户端程序监听mqtt消息

在Edge(192.168.0.105)端创建此程序

```
su #切换为root用户
cd /root/
touch /root/main.go
```

/root/main.go

```
package main

import (
	"encoding/json"
	"fmt"
	"github.com/kubeedge/kubeedge/cloud/pkg/devicecontroller/types"
	"github.com/yosssi/gmq/mqtt"
	"github.com/yosssi/gmq/mqtt/client"
)

func main() {
	fmt.Println("Get music list successfully")

	cli := client.New(&client.Options{
		// Define the processing of the error handler.
		ErrorHandler: func(err error) {
			fmt.Println(err)
		},
	})

	fmt.Println("Create mqtt client successfully")

	stopchan := make(chan int)
	// Terminate the Client.
	defer cli.Terminate()

	// Connect to the MQTT Server.
	err := cli.Connect(&client.ConnectOptions{
		Network:  "tcp",
		Address:  "localhost:1883",
		ClientID: []byte("receive-client"),
	})
	if err != nil {
		panic(err)
	}
	fmt.Println("Connect mqtt client successfully")

	err = cli.Subscribe(&client.SubscribeOptions{
		SubReqs: []*client.SubReq{
			{
				TopicFilter: []byte(`$hw/events/device/speaker-01/twin/update/document`),
				QoS:         mqtt.QoS0,
				// Define the processing of the message handler.
				Handler: func(topicName, message []byte) {
					Update := &types.DeviceTwinDocument{}
					err := json.Unmarshal(message, Update)
					if err != nil {
						fmt.Println("Unmarshal error", err)
						fmt.Printf("Unmarshal error: %v\n", err)
					}
					fmt.Printf("%+v", Update)
				},
			},
		},
	})
	fmt.Println("Subscribe mqtt topic successfully")

	<-stopchan
	if err != nil {
		panic(err)
	} else {
		fmt.Println("Connection successfully")
	}
}
```

运行此程序

```
go get github.com/yosssi/gmq/mqtt/client
go get github.com/yosssi/gmq/mqtt
go run /root/main.go
```

输出

```
Get music list successfully
Create mqtt client successfully
Connect mqtt client successfully
Subscribe mqtt topic successfully
```

## 9 查看运行效果

查看运行效果需要同时打开如下几个应用(终端建议VS Code Remote):

Cloud上:

1. pod日志终端

   ```
   kubectl logs -f kubeedge-web-app-76d4755f59-f5zbr
   ```

2. 网页`172.17.0.2`

Edge上:

1. Kubeedge日志

   ```
   tail -f /var/log/kubeedge/edgecore.log
   ```

2. MQTT客户端程序

   ```
   go run main.go
   ```

选中6号歌曲歌曲, 点击网页`Play`按钮, 依次产生如下结果:

Cloud上:

```
$ kubectl logs -f kubeedge-web-app-76d4755f59-f5zbr
2020/04/05 10:35:34 PlayTrack: 6
2020/04/05 10:35:34 Track [ 6 ] will be played on speaker speaker-01
```

Edge上:

```
$ tail -f /var/log/kubeedge/edgecore.log
I0405 18:37:47.912391    5635 eventbus.go:88] Success in pubMQTT with topic: $hw/events/device/speaker-01/twin/update/document
I0405 18:37:47.912567    5635 eventbus.go:88] Success in pubMQTT with topic: $hw/events/device/speaker-01/twin/update/delta
$ go run main.go
Get music list successfully
Create mqtt client successfully
Connect mqtt client successfully
Subscribe mqtt topic successfully
&{BaseMessage:{EventID: Timestamp:1586082823422} Twin:map[track:0xc000012d90]}&{BaseMessage:{EventID:307710b5-ef50-48be-abbd-f39ff079b42c Timestamp:1586082934968} Twin:map[track:0xc000012e50]}
```

如果在Edge上安装`sqlite3`还可以看到Kubeedge把消息存到数据库中.

```
$ apt-get install sqlite3
$ cat /etc/kubeedge/config/edgecore.yaml | grep datasource
datasource: /var/lib/kubeedge/edgecore.db
$ sqlite3 /var/lib/kubeedge/edgecore.db
SQLite version 3.22.0 2018-01-22 18:45:57
Enter ".help" for usage hints.
sqlite> .tables
device       device_attr  device_twin  meta       
sqlite> .header on
sqlite> .mode column
sqlite> SELECT * FROM device_twin;
id          deviceid    name        description  expected    actual      expected_meta                actual_meta  expected_version           actual_version  optional    attr_type   metadata  
----------  ----------  ----------  -----------  ----------  ----------  ---------------------------  -----------  -------------------------  --------------  ----------  ----------  ----------
4           speaker-01  track                    6                       {"timestamp":1586083067906}               {"cloud":106581,"edge":0}                  0           string      {}        
```

至此, 可以证明Kubeedge Cloud和Kubeedge Edge完成了通信.